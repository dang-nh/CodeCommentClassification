# Quick Reference Guide

## ğŸ¯ Your Questions Answered

### â“ Is max_len=512 enough?

**Answer: YES, but use 128 instead!**

```
Your Data Analysis:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Percentile      â”‚ Chars    â”‚ Tokens     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Mean            â”‚ 45       â”‚ 12         â”‚
â”‚ 95th            â”‚ 95       â”‚ 21         â”‚
â”‚ 99th            â”‚ 149      â”‚ 32         â”‚
â”‚ Max             â”‚ 1,321    â”‚ 281        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Coverage:
â€¢ max_len=128: covers 99.5% of data âœ… RECOMMENDED
â€¢ max_len=256: covers 99.7% of data
â€¢ max_len=512: covers 100% of data (OVERKILL)
```

---

### â“ What's the best model?

**Answer: DeBERTa-v3-base (microsoft/deberta-v3-base)**

```
Model Ranking (2024):
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ #  â”‚ Model            â”‚ F1 Est. â”‚ Speed    â”‚ Best For   â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ¥‡ â”‚ DeBERTa-v3       â”‚ 0.87-90 â”‚ â­â­â­   â”‚ Best Score â”‚
â”‚ ğŸ¥ˆ â”‚ GraphCodeBERT    â”‚ 0.82-88 â”‚ â­â­â­â­ â”‚ Code Tasks â”‚
â”‚ ğŸ¥‰ â”‚ CodeBERT         â”‚ 0.80-85 â”‚ â­â­â­â­ â”‚ Baseline   â”‚
â”‚    â”‚ ModernBERT       â”‚ 0.78-84 â”‚ â­â­â­â­â­â”‚ Speed      â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### â“ What's the best config?

**Answer: Use `configs/dl_best_config.yaml`**

```yaml
# Key Optimizations:
model_name: "microsoft/deberta-v3-base"  # Best model
max_len: 128                              # â† Changed from 512!
batch_size: 32                            # Optimal
lr: 0.00002                               # 2e-5
epochs: 10                                # Enough for convergence
use_single_split: false                   # 5-fold CV for comparison
peft.enabled: false                       # Full SFT for best performance
loss_type: "asl"                          # Handles imbalance
```

---

## ğŸš€ How to Run

### 1. Best Performance (DeBERTa-v3)
```bash
python dl_solution.py configs/dl_best_config.yaml
```
**Expected:** F1 = 0.87-0.90, ~2 hours, 8GB GPU

### 2. Code-Specific (GraphCodeBERT)
```bash
python dl_solution.py configs/dl_graphcodebert_optimized.yaml
```
**Expected:** F1 = 0.82-0.88, ~1.5 hours, 6GB GPU

### 3. Fast Baseline
```bash
python dl_solution.py configs/dl_optimized.yaml
```
**Expected:** F1 = 0.78-0.84, ~1 hour, 5GB GPU

---

## ğŸ“Š Why 5-Fold CV > Single Split?

```
Your Dataset Issues:
â”œâ”€ Only 6,738 samples (limited)
â”œâ”€ 16 labels (multi-label)
â”œâ”€ 22x imbalance (1,712 vs 77 samples)
â””â”€ Rare labels: only 15-25 test samples in single split âŒ

5-Fold CV Benefits:
â”œâ”€ Uses ALL data for training AND validation âœ…
â”œâ”€ Mean Â± Std metrics (reliability) âœ…
â”œâ”€ Better for rare labels âœ…
â””â”€ Confident model comparison âœ…

Recommendation:
1. Use 5-fold CV for model comparison
2. Use single split for final model training
```

---

## ğŸ“ Web Research Summary

### Latest Best Practices (2024):

**For Multi-Label Classification:**
- âœ… DeBERTa-v3 outperforms BERT/RoBERTa
- âœ… Learning rate: 2e-5 for Full SFT, 3e-4 for LoRA
- âœ… Batch size: 32 is optimal
- âœ… Cosine scheduler with 10% warmup
- âœ… Asymmetric Loss for imbalanced labels
- âœ… Early stopping patience: 5 epochs

**For Code Tasks:**
- âœ… GraphCodeBERT: best code-specific model
- âœ… CodeBERT: solid baseline
- âœ… DeBERTa-v3: often beats code-specific models

**For Limited Data:**
- âœ… Use stratified k-fold CV
- âœ… LoRA for parameter efficiency
- âœ… Early stopping to prevent overfitting
- âœ… Per-label threshold optimization

---

## ğŸ“ˆ Expected Results

```
Experiment              â”‚ F1 Score    â”‚ Time    â”‚ Memory
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€
DeBERTa-v3 (Full SFT)   â”‚ 0.87-0.90   â”‚ ~2h     â”‚ 8GB
GraphCodeBERT (LoRA)    â”‚ 0.82-0.88   â”‚ ~1.5h   â”‚ 6GB
CodeBERT (Full SFT)     â”‚ 0.80-0.85   â”‚ ~1.2h   â”‚ 6-8GB
ModernBERT              â”‚ 0.78-0.84   â”‚ ~1h     â”‚ 5GB
```

---

## ğŸ”§ Config Options Explained

### Training Mode:
```yaml
peft:
  enabled: false  # Full SFT - all parameters trainable (best performance)
  enabled: true   # LoRA - efficient (good performance, less memory)
```

### Evaluation Strategy:
```yaml
use_single_split: false  # 5-fold CV (for model comparison)
use_single_split: true   # Single 80/20 (for final training)
```

### Max Length:
```yaml
max_len: 128  # Fast, covers 99.5% data âœ… RECOMMENDED
max_len: 256  # Conservative, covers 99.7% data
max_len: 512  # Slow, covers 100% data (overkill)
```

---

## ğŸ¯ Workflow

```
Phase 1: Model Comparison
â”œâ”€ Run DeBERTa-v3 (5-fold CV)
â”œâ”€ Run GraphCodeBERT (5-fold CV)
â”œâ”€ Run CodeBERT (5-fold CV)
â””â”€ Compare: F1_mean Â± F1_std

Phase 2: Final Model
â”œâ”€ Choose best model from Phase 1
â”œâ”€ Set use_single_split: true
â”œâ”€ Train on 80% data
â”œâ”€ Evaluate on 20% test
â””â”€ Save model for production
```

---

## ğŸ’¡ Pro Tips

1. **Start with `dl_best_config.yaml`** - highest performance
2. **Use 5-fold CV first** - reliable comparison
3. **Monitor GPU** with `nvidia-smi`
4. **Check per-label F1** - overall F1 can be misleading
5. **Use LoRA if OOM** - reduces memory by 50%
6. **Reduce max_len to 64** - for very fast experiments
7. **Ensemble 5 folds** - average predictions for production

---

## ğŸ“ All Configs at a Glance

```
configs/
â”œâ”€ dl_best_config.yaml               â­ DeBERTa-v3, max_len=128, Full SFT
â”œâ”€ dl_graphcodebert_optimized.yaml   ğŸ“ GraphCodeBERT, LoRA
â”œâ”€ dl_single_split.yaml              âš¡ Single split example
â”œâ”€ dl_optimized.yaml                 ğŸ  Original (ModernBERT)
â””â”€ dl_graphcodebert.yaml             ğŸ“ Original GraphCodeBERT
```

---

## ğŸ› Troubleshooting

**Out of Memory?**
- Reduce batch_size: 32 â†’ 16 â†’ 8
- Use LoRA: set `peft.enabled: true`
- Reduce max_len: 128 â†’ 64

**Training Too Slow?**
- Increase batch_size if memory allows
- Reduce max_len: 128 â†’ 64
- Use single split instead of 5-fold

**Poor Performance?**
- Increase epochs: 10 â†’ 15
- Try Full SFT instead of LoRA
- Check if max_len is too small

---

## âœ… Ready to Run!

```bash
# Quick test (5 minutes)
python dl_solution.py configs/dl_single_split.yaml

# Best model (2 hours)
python dl_solution.py configs/dl_best_config.yaml

# All comparisons (5 hours)
for config in configs/dl_best_config.yaml \
              configs/dl_graphcodebert_optimized.yaml \
              configs/dl_optimized.yaml; do
    python dl_solution.py $config
done
```

**Just specify your conda environment and let's go! ğŸš€**

