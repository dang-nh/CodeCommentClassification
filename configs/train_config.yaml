model_name: "roberta-large"
tokenizer_name: "roberta-large"
num_labels: 19
max_len: 128

# L40S 46GB: bf16 ổn định, tận dụng VRAM; có thể dùng "fp16" hoặc "fp32"
precision: "fp32"
gradient_checkpointing: false
use_single_split: false           # <-- 5-fold CV

cv:
  n_splits: 5

text_features:
  include_lang: true
  include_class_id: true
  template: "[{lang}] {class_id} | {sentence}"

peft:
  enabled: true                  # full finetune; bật true nếu muốn LoRA
  r: 32
  alpha: 64
  dropout: 0.05
  target_modules: ["query", "key", "value", "dense"]

# Loss: 'asl' (mặc định) | 'bce' (pos_weight) | 'focal'
loss_type: "asl"
loss_params:
  gamma_pos: 0
  gamma_neg: 4
  clip: 0.05

# Sampler giúp nhãn hiếm: bật nếu macro/samples F1 thấp vì lệch nhãn
sampler:
  enabled: true
  type: "weighted"               # hiện hỗ trợ "weighted"
  smooth_eps: 1e-3

train_params:
  batch_size: 64                # L40S 46GB với roberta-large + seq=128 OK
  eval_batch_size: 64
  grad_accum: 1
  epochs: 100
  lr: 0.00005
  scheduler: "cosine"            # "cosine" | "linear"
  warmup: 0.10
  weight_decay: 0.01
  seed: 42
  num_workers: 4

data:
  raw_path: "data/raw/sentences.csv"
  processed_path: "data/processed/"
  expand_to_19_classes: true     # tách summary/usage/expand theo ngôn ngữ (Java/Python)
  label_list:
    # 7 Java
    - summary_java
    - pointer
    - deprecation
    - rational
    - ownership
    - usage_java
    - expand_java
    # 5 Python
    - summary_python
    - parameters
    - usage_python
    - developmentnotes
    - expand_python
    # 7 Pharo
    - keymessages
    - intent
    - classreferences
    - example
    - keyimplementationpoints
    - responsibilities
    - collaborators

eval:
  metrics: ["precision", "recall", "f1", "roc_auc", "pr_auc"]
  threshold_search: true         # tối ưu threshold mỗi epoch (F1(samples))

logging:
  output_dir: "runs/roberta_large_ccc_19cv_fold_validation"
  save_best: true
  early_stop: 10
  tensorboard: false

final_training:                   # (tùy chọn) train lại trên full data sau CV
  enabled: true
  epochs: 50
  warmup: 0.10
  save_path: "runs/roberta_large_ccc_19cv_fold_validation/final_model.pt"

cv:
  n_splits: 5
  start_fold: 1             # <-- chạy tiếp từ fold 1 (0-based). Có thể đổi tuỳ bạn
  skip_if_checkpoint_exists: true  # <-- tự động bỏ qua fold đã có ckpt
