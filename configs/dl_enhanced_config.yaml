# Enhanced DL Config - Optimized to Beat STACC (F1=0.744)
# Key improvements: Multi-pooling, Multi-Sample Dropout, R-Drop, EMA, Combined Loss, Layer-wise LR

model_name: "microsoft/codebert-base"
tokenizer_name: "microsoft/codebert-base"
num_labels: 16
max_len: 512

data:
  raw_path: "./data/raw/sentences.csv"

# Enhanced Architecture
use_multi_sample_dropout: true
use_attention_pooling: true
pooling_strategy: "concat_all"  # CLS + Mean + Max + Attention

# PEFT Configuration
peft:
  enabled: true
  r: 16  # Increased from 8
  alpha: 32  # Increased from 16
  dropout: 0.05

# Enhanced Loss Configuration
loss_type: "combined"  # ASL + Focal + BCE
label_smoothing: 0.1
use_rdrop: true
rdrop_alpha: 5.0

loss_params:
  gamma_pos: 0
  gamma_neg: 4
  clip: 0.05

# Training Configuration
train_params:
  batch_size: 16  # Adjusted for memory
  epochs: 10  # Increased for better convergence
  lr: 2e-5  # Similar to STACC's 1.7e-5
  weight_decay: 0.01
  warmup: 0.1
  seed: 42
  scheduler: "cosine"

# Advanced Optimizations
use_layerwise_lr: true
lr_decay: 0.95  # Layer-wise learning rate decay

use_ema: true
ema_decay: 0.999

num_threshold_search: 200  # More granular than default 100

# Precision
precision: "fp16"

# Cross-Validation or Single Split
use_single_split: true  # Set to false for 5-fold CV

# Logging
logging:
  output_dir: "./runs/enhanced_codebert_vs_stacc"
  early_stop: 5
  save_model: true

